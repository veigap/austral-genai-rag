# RAG (Retrieval-Augmented Generation) Examples

This directory contains **four progressive examples** demonstrating RAG patterns with both **Elasticsearch** (full-text search) and **ChromaDB** (vector/semantic search).

## ðŸ“‹ Overview

All examples require a `.env` file with your Google API key:
```bash
GOOGLE_API_KEY=your-key-here
ELASTICSEARCH_URL=http://localhost:9200
CHROMA_URL=http://localhost:8000
CHROMA_EMBEDDING_FUNCTION=default  # optional
```

See [SETUP.md](../../SETUP.md) for details.

## ðŸ“ Files

### `case1-direct-rag-example.ts` â­ **Simplest RAG**

A straightforward RAG example **without MCP or agents**. Perfect for learning the RAG pattern.

**What it does:**
1. User asks a question
2. Searches Elasticsearch directly
3. Passes results to AI model
4. AI generates a helpful, contextual response

**Architecture:**
```
User Question
     â†“
Elasticsearch Search  (retrieval)
     â†“
Search Results (context)
     â†“
AI Model (generation)
     â†“
Helpful Answer
```

**Features:**
- ðŸ” Debug logging shows ES queries and results
- ðŸ“Š Relevance scores displayed
- ðŸŽ¯ Simple, linear flow

**Run:**
```bash
# 1. Create .env with your Google API key (one-time)
echo "GOOGLE_API_KEY=your-key-here" > .env

# 2. Start Elasticsearch
yarn elasticsearch:start

# 3. Run the example (data auto-loads)
yarn rag:case1
```

**Example output:**
```
ðŸ” User question: "I need a powerful laptop for work"

ðŸ“¦ Searching product catalog...

ðŸ“¤ Elasticsearch Query:
{
  "index": "products",
  "query": {
    "multi_match": {
      "query": "powerful laptop work",
      "fields": ["name", "description", "category"]
    }
  },
  "size": 5
}

ðŸ“¥ Elasticsearch Results:
   Total hits: 3
   Max score: 2.45
   1. [Score: 2.45] Dell XPS 15 ($1299.99)
   2. [Score: 1.87] MacBook Pro 14" ($1999.99)
   3. [Score: 1.23] HP Spectre x360 ($1399.99)

âœ… Found 3 products

ðŸ¤– Generating AI response...

ðŸ’¬ AI Assistant:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Based on your need for a powerful work laptop, I'd recommend:

1. **Dell XPS 15** ($1,299.99) - Best value!
   Perfect balance of power and price with i7 processor and 16GB RAM.
   Excellent for demanding work tasks. We have 15 units in stock.

2. **MacBook Pro 14"** ($1,999.99)
   If you're in the Apple ecosystem, this is excellent with the
   M2 Pro chip and 16GB RAM. Premium build quality. 8 in stock.

All are in stock and ready to ship!
```

### `case2-agent-rag-example.ts` - **Agent with Tools** â­

Uses LangChain agent with a tool to search Elasticsearch. The agent decides when to use the search tool.

**What it does:**
1. Agent receives customer question
2. Agent decides to use the search_products tool
3. Tool searches Elasticsearch (with debug logging)
4. Agent generates helpful recommendation

**Architecture:**
```
User Question
     â†“
Agent (decides to use tool)
     â†“
search_products Tool â†’ Elasticsearch
     â†“
Search Results (with debug info)
     â†“
Agent (generates answer)
     â†“
Helpful Response
```

**Features:**
- ðŸ¤– Agent intelligence
- ðŸ”§ Custom LangChain tools
- ðŸ” Debug logging for all searches
- ðŸ“Š Shows ES queries and relevance scores

**Run:**
```bash
# 1. Start Elasticsearch
yarn elasticsearch:start

# 2. Run the agent example
yarn rag:case2
```

**Benefits over case1:**
- âœ… Agent decides when to search
- âœ… Can ask clarifying questions
- âœ… Multi-turn conversations
- âœ… Tool calling framework

### `case3-agent-mcp-example.ts` - **Agent with MCP** â­â­

Agent that uses the MCP Elasticsearch server through the `@langchain/mcp-adapters`. Best of both worlds!

**What it does:**
1. Agent receives customer question
2. Agent decides to use MCP tool (auto-discovered)
3. Tool calls MCP server (stdio)
4. MCP server searches Elasticsearch (with debug logging)
5. Agent generates recommendation

**Architecture:**
```
User Question
     â†“
Agent (decides to use tool)
     â†“
MCP Adapter (auto-discovers tools)
     â†“
stdio â†’ MCP Server (separate process)
     â†“
Elasticsearch (with debug logging)
     â†“
Search Results
     â†“
Agent (generates answer)
     â†“
Helpful Response
```

**Features:**
- ðŸ—ï¸ Microservices architecture
- ðŸ“¡ stdio-based MCP communication
- ðŸ”„ Decoupled components
- ðŸ” Debug logging in MCP server
- âœ¨ Auto-discovered tools (no manual definitions!)

**Run:**
```bash
# 1. Start Elasticsearch
yarn elasticsearch:start

# 2. Start MCP server (Terminal 2)
yarn mcp:elasticsearch

# 3. Run agent (Terminal 3)
yarn rag:case3
```

**Benefits:**
- âœ… Agent intelligence + MCP protocol
- âœ… Microservices architecture
- âœ… Multiple tools (search, get indices, index documents)
- âœ… Scalable and modular
- âœ… Production-ready pattern
- âœ… Auto-discovered tools via `@langchain/mcp-adapters`


Agent that uses the official `chroma-mcp` Python server for semantic similarity search.

**What it does:**
1. Agent receives customer question
2. Agent decides to use MCP tool (auto-discovered)
3. Tool calls chroma-mcp server (Python, stdio, via uvx)
4. MCP server performs semantic search on ChromaDB
5. Agent generates recommendation based on similarity

**Architecture:**
```
User Question
     â†“
Agent (decides to use tool)
     â†“
MCP Adapter (auto-discovers tools)
     â†“
stdio â†’ chroma-mcp (Python, official)
     â†“
ChromaDB (semantic/vector search)
     â†“
Search Results (similarity scores)
     â†“
Agent (generates answer)
     â†“
Helpful Response
```

**Features:**
- ðŸ§® Semantic similarity search (not keyword matching)
- ðŸ Official `chroma-mcp` Python package (via uvx)
- ðŸ“ Embeddings: MiniLM-L6-v2 (default, free, local)
- ðŸ”§ Configurable embedding models (OpenAI, Cohere, Jina, etc.)
- ðŸ—ï¸ Microservices architecture
- âœ¨ Auto-discovered tools

**Run:**
```bash
# 1. Install chroma-mcp (one-time)
pip3 install chroma-mcp

# 2. Start ChromaDB
yarn chroma:start

# 3. Explore data (optional)
yarn chroma:console

# 4. Run agent
```

**Benefits:**
- âœ… Semantic search (understands meaning, not just keywords)
- âœ… Official MCP server from Chroma team
- âœ… Built-in embeddings (no separate service needed)
- âœ… Configurable embedding models
- âœ… Interactive console for data exploration
- âœ… Perfect for AI-first applications

**Embedding Configuration (Educational):**

The example shows how to configure embeddings:

```bash
# In .env
GOOGLE_API_KEY=your-key  # Required: For AI model (Google Gemini)
CHROMA_EMBEDDING_FUNCTION=default  # Optional: MiniLM-L6-v2 (free, local)

# Advanced: Use different embeddings (optional)
# CHROMA_EMBEDDING_FUNCTION=openai
# CHROMA_OPENAI_API_KEY=sk-your-embedding-key
```

**Important:** 
- **GOOGLE_API_KEY** = AI text generation (Google Gemini) - **Required**
- **CHROMA_*_API_KEY** = Document embeddings - **Optional** (default is free)

## ðŸ“Š Comparison

| Feature | Case 1 | Case 2 | Case 3 | Case 4 |
|---------|--------|--------|--------|--------|
| **Name** | Direct RAG | Agent + ES | Agent + MCP + ES | Agent + MCP + Chroma |
| **Search Type** | Full-text | Full-text | Full-text | **Semantic** |
| **Simplicity** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ |
| **Dependencies** | ES + AI | ES + AI + Agent | ES + AI + MCP + Agent | Chroma + AI + MCP + Agent |
| **Setup** | Direct | Direct + Tool | MCP server | MCP server + uvx |
| **MCP Protocol** | âŒ No | âŒ No | âœ… Yes | âœ… Yes |
| **Tool calling** | âŒ No | âœ… Yes | âœ… Yes (auto) | âœ… Yes (auto) |
| **Agent reasoning** | âŒ No | âœ… Yes | âœ… Yes | âœ… Yes |
| **Microservices** | âŒ No | âŒ No | âœ… Yes | âœ… Yes |
| **Embeddings** | âŒ No | âŒ No | âŒ No | âœ… Yes (built-in) |
| **Console** | Kibana | Kibana | Kibana | Custom CLI |
| **Best for** | Learning RAG | Simple assistant | Production (ES) | Production (Vector) |
| **Code lines** | ~120 | ~175 | ~110 | ~130 |
| **Terminals needed** | 2 | 2 | 3 | 2 |

## ðŸŽ¯ Which to Use?

### Use **Case 1** when:
âœ… **Learning RAG concepts**
âœ… Simple product search/recommendations
âœ… Minimal dependencies
âœ… Fast prototyping
âœ… Embedding RAG in existing apps
âœ… You control the search logic
âœ… Want to see exactly how RAG works

### Use **Case 2** when:
âœ… **Building a smart assistant**
âœ… Agent should decide when to search
âœ… Natural conversation flow
âœ… Product recommendations
âœ… Customer support chatbot
âœ… Want tool calling but no MCP overhead
âœ… Multi-turn conversations

### Use **Case 3** when: â­ **Recommended for Production (Full-Text)**
âœ… **Building production applications**
âœ… Want agent intelligence + MCP benefits
âœ… Microservices architecture
âœ… Need to scale search independently
âœ… Multiple clients accessing same data
âœ… Best practices for enterprise apps
âœ… Future-proof with MCP standard
âœ… Auto-discovered tools
âœ… Full-text search + filters needed

### Use **Case 4** when: â­ **Recommended for AI-First Apps**
âœ… **Semantic similarity is more important than keyword matching**
âœ… Building AI-first applications (recommendations, similar items)
âœ… Want embeddings without external services
âœ… Need "find similar" functionality
âœ… User queries are natural language
âœ… Configurable embedding models
âœ… Exploring vector databases
âœ… Microservices with semantic search

### Elasticsearch (Cases 1-3) vs ChromaDB (Case 4):

**Choose Elasticsearch when:**
- Keywords and exact matches matter
- Need complex filters (price range, categories)
- Traditional search engine behavior
- SQL-like queries
- Full-text search is primary use case

**Choose ChromaDB when:**
- Meaning matters more than exact words  
- "Find similar items" is key
- Natural language queries
- AI/ML is core to your app
- Semantic search is primary use case

## ðŸ“– RAG Pattern Explained

**RAG = Retrieval-Augmented Generation**

```typescript
// 1. RETRIEVAL: Get relevant documents
const searchResults = await elasticsearch.search(userQuery);

// 2. AUGMENTATION: Add context to prompt
const prompt = `
Question: ${userQuery}
Context: ${searchResults}
Please answer using this context...
`;

// 3. GENERATION: AI creates answer
const answer = await aiModel.invoke(prompt);
```

**Why RAG?**
- âœ… AI uses **your data** (not just training data)
- âœ… **Accurate** answers based on real documents
- âœ… **Up-to-date** information from your database
- âœ… **Transparent** - see which documents were used
- âœ… **Reduces hallucinations** - grounds AI in facts

## ðŸš€ Quick Start

### Path A: Elasticsearch (Full-Text Search)
```bash
# 1. Setup environment (one-time)
cat > .env << 'EOF'
GOOGLE_API_KEY=your-key-here
ELASTICSEARCH_URL=http://localhost:9200
EOF

# 2. Start Elasticsearch
yarn elasticsearch:start

# 3. Run examples
yarn rag:case1  # Direct RAG
yarn rag:case2  # Agent + Tools
yarn rag:case3  # Agent + MCP (need separate terminal for MCP server)
```

### Path B: ChromaDB (Semantic Search)
```bash
# 1. Setup environment (one-time)
cat > .env << 'EOF'
GOOGLE_API_KEY=your-key-here
CHROMA_URL=http://localhost:8000
EOF

# 2. Install chroma-mcp (one-time)
pip3 install chroma-mcp

# 3. Start ChromaDB
yarn chroma:start

# 4. Explore data
yarn chroma:console

# 5. Run example
  # Agent + MCP + Semantic Search
```

## ðŸ” Debug Output

All examples now show detailed debug information:

**Case 1 & Case 2:**
- ðŸ“¤ Elasticsearch query sent
- ðŸ“¥ Total hits and max score
- ðŸ“‹ Each matching document with score

**Case 3:**
- Same debug output from the MCP server
- Visible in the MCP server terminal (stderr)

Example:
```
ðŸ“¤ Elasticsearch Query:
{
  "index": "products",
  "query": {
    "multi_match": {
      "query": "laptop",
      "fields": ["name", "description", "category"]
    }
  },
  "size": 5
}

ðŸ“¥ Elasticsearch Results:
   Total hits: 3
   Max score: 2.45
   1. [Score: 2.45] Dell XPS 15 ($1299.99)
   2. [Score: 1.87] MacBook Pro 14" ($1999.99)
   3. [Score: 1.23] HP Spectre x360 ($1399.99)
```

## ðŸ“¦ Sample Data

All examples use the **same product catalog** from `data/products.json`:
- **Laptops**: MacBook Pro 16", Dell XPS 13, Lenovo ThinkPad X1
- **Accessories**: Apple Magic Mouse, Logitech MX Master 3

**Key Points:**
- âœ… Data is shared across Elasticsearch and ChromaDB
- âœ… Automatically loaded when you start services
- âœ… Consistent structure for fair comparison
- âœ… Easy to modify and experiment with

**Explore the data:**
```bash
# Elasticsearch (Kibana UI)
open http://localhost:5601/app/dev_tools#/console

# ChromaDB (Interactive CLI)
yarn chroma:console
```

## âš™ï¸ Customization

### Add your own data:

```typescript
import 'dotenv/config';
import { Client } from '@elastic/elasticsearch';

const client = new Client({ 
    node: process.env.ELASTICSEARCH_URL || 'http://localhost:9200' 
});

await client.index({
    index: 'products',
    document: {
        name: 'Your Product',
        category: 'Category',
        price: 999,
        description: 'Description',
        stock: 10
    }
});
```

### Customize the AI prompt:

Edit any example to change how the AI responds:

```typescript
const prompt = `You are a [YOUR ROLE]. 
The customer asked: "${userQuery}"

Products: ${context}

Respond in a [YOUR STYLE] manner...`;
```

## ðŸ’¡ Tips

1. **Better Search**: Use more specific fields in `multi_match`
2. **Better AI**: Be specific in your prompts
3. **Better Results**: Add more product metadata
4. **Performance**: Index more documents for better relevance
5. **Debug**: All examples show ES queries - use this to tune relevance
6. **Kibana**: Use http://localhost:5601 to explore your data

## ðŸ“š Learn More

- [What is RAG?](https://www.promptingguide.ai/techniques/rag)
- [Elasticsearch Query DSL](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html)
- [Prompt Engineering](https://www.promptingguide.ai/)
- [MCP Specification](https://spec.modelcontextprotocol.io/)
- [LangChain MCP Adapters](https://js.langchain.com/docs/integrations/tools/mcp)

## ðŸ”’ Security

- âœ… Never commit `.env` to git (already in `.gitignore`)
- âœ… Use environment variables for API keys
- âœ… `dotenv` auto-loads `.env` on script start
- âœ… See [SETUP.md](../../SETUP.md) for best practices
